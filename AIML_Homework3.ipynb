{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9QcGnGPdX2C",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "**Install requirements**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9O3aM3Tb28q",
        "colab_type": "code",
        "outputId": "36326f3f-53e5-417b-c996-43ba42ec5bc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 639
        }
      },
      "source": [
        "!pip3 install 'Pillow==6.1'\n",
        "!pip3 install 'torch==1.3.1'\n",
        "!pip3 install 'torchvision==0.4.2'\n",
        "!pip3 install 'Pillow-SIMD'\n",
        "!pip3 install 'tqdm'\n",
        "#!pip install --upgrade pillow"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting Pillow==6.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/41/db6dec65ddbc176a59b89485e8cc136a433ed9c6397b6bfe2cd38412051e/Pillow-6.1.0-cp36-cp36m-manylinux1_x86_64.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 9.7MB/s \n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Pillow\n",
            "  Found existing installation: Pillow 6.2.2\n",
            "    Uninstalling Pillow-6.2.2:\n",
            "      Successfully uninstalled Pillow-6.2.2\n",
            "Successfully installed Pillow-6.1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch==1.3.1 in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.3.1) (1.17.5)\n",
            "Requirement already satisfied: torchvision==0.4.2 in /usr/local/lib/python3.6/dist-packages (0.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==0.4.2) (1.17.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==0.4.2) (1.12.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.4.2) (6.1.0)\n",
            "Requirement already satisfied: torch==1.3.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.4.2) (1.3.1)\n",
            "Collecting Pillow-SIMD\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b1/19/b7043190f481abb94dcdd1e69c4432432aaa73455cf1128eae39b8eb2518/Pillow-SIMD-6.0.0.post0.tar.gz (621kB)\n",
            "\u001b[K     |████████████████████████████████| 624kB 8.9MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: Pillow-SIMD\n",
            "  Building wheel for Pillow-SIMD (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Pillow-SIMD: filename=Pillow_SIMD-6.0.0.post0-cp36-cp36m-linux_x86_64.whl size=1062936 sha256=8023e44aeeda53944fd0d80bebd7d5d5a665a0152ccd431b10021856cc7aaaf8\n",
            "  Stored in directory: /root/.cache/pip/wheels/06/60/65/cc9afa345ccbf10a34cc208266b992941a8608010b592f43d1\n",
            "Successfully built Pillow-SIMD\n",
            "Installing collected packages: Pillow-SIMD\n",
            "Successfully installed Pillow-SIMD-6.0.0.post0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fo942LMOdlh4",
        "colab_type": "text"
      },
      "source": [
        "**Import libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DokFOdD1dJEl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import logging\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torch.backends import cudnn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "from torch.autograd import Function\n",
        "\n",
        "try:\n",
        "    from torch.hub import load_state_dict_from_url\n",
        "except ImportError:\n",
        "    from torch.utils.model_zoo import load_url as load_state_dict_from_url"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8GddHWzdKIk",
        "colab_type": "text"
      },
      "source": [
        "**ReverseLayer**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "090x1d1tdK-J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReverseLayerF(Function):\n",
        "    # Forwards identity\n",
        "    # Sends backward reversed gradients\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, alpha):\n",
        "        ctx.alpha = alpha\n",
        "        return x.view_as(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        output = grad_output.neg() * ctx.alpha\n",
        "        return output, None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CB4F9hN0QJTa",
        "colab_type": "text"
      },
      "source": [
        "**AlexNetDANN**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kpoZrLzQJ8-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "__all__ = ['AlexNet', 'alexnet']\n",
        "\n",
        "model_urls = {\n",
        "    'alexnet': 'https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth',\n",
        "}\n",
        "\n",
        "class AlexNetDANN(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes=1000):\n",
        "        super(AlexNetDANN, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        )\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(256 * 6 * 6, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "        \n",
        "        self.domain = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(256 * 6 * 6, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4096, 2),\n",
        "        )\n",
        "        \n",
        "\n",
        "    def forward(self, x, alpha=None):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        if alpha == None:\n",
        "          x = self.classifier(x)\n",
        "        else:\n",
        "          x = ReverseLayerF.apply(x, alpha)\n",
        "          x = self.domain(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def alexnetDANN(pretrained=False, progress=True, **kwargs):\n",
        "    \"\"\"AlexNet model architecture from the\n",
        "    `\"One weird trick...\" <https://arxiv.org/abs/1404.5997>`_ paper.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    model = AlexNetDANN(**kwargs)\n",
        "    if pretrained:\n",
        "        state_dict = load_state_dict_from_url(model_urls['alexnet'],\n",
        "                                              progress=progress)\n",
        "        model.load_state_dict(state_dict, strict=False)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIDLJuIXK_vh",
        "colab_type": "text"
      },
      "source": [
        "**Set Arguments**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5PkYfqfK_SA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEVICE = 'cuda' # 'cuda' or 'cpu'\n",
        "\n",
        "BATCH_SIZE = 256     # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing\n",
        "                     # the batch size, learning rate should change by the same factor to have comparable results\n",
        "\n",
        "MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
        "WEIGHT_DECAY = 5e-5  # Regularization, you can keep this at the default\n",
        "\n",
        "NUM_EPOCHS = 30      # Total number of training epochs (iterations over dataset)\n",
        "GAMMA = 0.1          # Multiplicative factor for learning rate step-down\n",
        "\n",
        "LOG_FREQUENCY = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gwii0TBHvzh",
        "colab_type": "text"
      },
      "source": [
        "**Define Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUDdw4j2H0Mc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define transforms for training phase\n",
        "train_transform = transforms.Compose([transforms.Resize(256),      \n",
        "                                      transforms.CenterCrop(224), \n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])\n",
        "# Define transforms for the evaluation phase\n",
        "eval_transform = transforms.Compose([transforms.Resize(256),\n",
        "                                      transforms.CenterCrop(224),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))                                    \n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qYIHPzYLY7i",
        "colab_type": "text"
      },
      "source": [
        "**Prepare Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfVq_uDHLbsR",
        "colab_type": "code",
        "outputId": "fa471232-d946-4d2d-835f-7f1ccc966c16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "# Clone github repository with data\n",
        "if not os.path.isdir('./Homework3-PACS'):\n",
        "  !git clone https://github.com/MachineLearning2020/Homework3-PACS\n",
        "\n",
        "P_DIR = 'Homework3-PACS/PACS/photo'\n",
        "A_DIR = 'Homework3-PACS/PACS/art_painting'\n",
        "C_DIR = 'Homework3-PACS/PACS/cartoon'\n",
        "S_DIR = 'Homework3-PACS/PACS/sketch'\n",
        "\n",
        "# Prepare Pytorch train/test Datasets\n",
        "train_dataset = torchvision.datasets.ImageFolder(P_DIR, transform=train_transform)\n",
        "test_dataset = torchvision.datasets.ImageFolder(A_DIR, transform=eval_transform)\n",
        "val1_dataset = torchvision.datasets.ImageFolder(C_DIR, transform=eval_transform)\n",
        "val2_dataset = torchvision.datasets.ImageFolder(S_DIR, transform=eval_transform)\n",
        "\n",
        "print('Train Dataset: {}'.format(len(train_dataset)))\n",
        "print('Test Dataset: {}'.format(len(test_dataset)))\n",
        "print('Val 1 Dataset: {}'.format(len(val1_dataset)))\n",
        "print('Val 2 Dataset: {}'.format(len(val2_dataset)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Homework3-PACS'...\n",
            "remote: Enumerating objects: 3, done.\u001b[K\n",
            "remote: Counting objects:  33% (1/3)\u001b[K\rremote: Counting objects:  66% (2/3)\u001b[K\rremote: Counting objects: 100% (3/3)\u001b[K\rremote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 10032 (delta 0), reused 3 (delta 0), pack-reused 10029\u001b[K\n",
            "Receiving objects: 100% (10032/10032), 174.13 MiB | 33.37 MiB/s, done.\n",
            "Checking out files: 100% (9993/9993), done.\n",
            "Train Dataset: 1670\n",
            "Test Dataset: 2048\n",
            "Val 1 Dataset: 2344\n",
            "Val 2 Dataset: 3929\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYEDQ7Z21ldN",
        "colab_type": "text"
      },
      "source": [
        "**Prepare Dataloaders**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VriRw8SI1nle",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dataloaders iterate over pytorch datasets and transparently provide useful functions (e.g. parallelization and shuffling)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "val1_dataloader = DataLoader(val1_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "val2_dataloader = DataLoader(val2_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEyL3H_R4qCf",
        "colab_type": "text"
      },
      "source": [
        "**Normal Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sjq00G94tSc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def training_phase(rates, steps, train_dataloader, val1_dataloader, val2_dataloader):\n",
        "  best_accuracy = -1\n",
        "  for lr in rates:\n",
        "    for ss in steps: \n",
        "      print(\"------------------------------------------------------\")\n",
        "      print(\"Start training with LR: \" + str(lr) + \" and SS: \" + str(ss))\n",
        "      print(\"------------------------------------------------------\")\n",
        "      net = alexnetDANN(pretrained=True)\n",
        "      net.classifier[6] = nn.Linear(4096, 7)\n",
        "\n",
        "      criterion = nn.CrossEntropyLoss() # for classification, we use Cross Entropy\n",
        "      parameters_to_not_optimize = net.domain.parameters()\n",
        "      for p in parameters_to_not_optimize:\n",
        "        p.requires_grad = False\n",
        "      parameters_to_optimize = net.parameters() # In this case we optimize over all the parameters of AlexNet\n",
        "      optimizer = optim.SGD(parameters_to_optimize, lr=lr, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "      scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=ss, gamma=GAMMA)\n",
        "\n",
        "      net = net.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n",
        "      cudnn.benchmark # Calling this optimizes runtime\n",
        "\n",
        "      current_step = 0      \n",
        "      for epoch in range(NUM_EPOCHS):\n",
        "        print('Starting epoch {}/{}, LR = {}, SS = {}'.format(epoch+1, NUM_EPOCHS, lr, ss))\n",
        "        net = net.to(DEVICE)\n",
        "        net.train(True)\n",
        "        for images, labels in train_dataloader:\n",
        "          images = images.to(DEVICE)\n",
        "          labels = labels.to(DEVICE)\n",
        "          optimizer.zero_grad() # Zero-ing the gradients\n",
        "          net.train() # Sets module in training mode\n",
        "          outputs = net(images) # Forward pass to the network\n",
        "          loss = criterion(outputs, labels)\n",
        "          if math.isnan(loss):\n",
        "            print (\"Loss is NaN!\")\n",
        "            break\n",
        "          loss.backward()  # backward pass: computes gradients\n",
        "          optimizer.step() # update weights based on accumulated gradients\n",
        "\n",
        "          if current_step % LOG_FREQUENCY == 0:\n",
        "            print('Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "          current_step += 1\n",
        "        if math.isnan(loss):\n",
        "          break\n",
        "        scheduler.step() # Step the scheduler\n",
        "\n",
        "        net = net.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n",
        "        net.train(False) # Set Network to evaluation mode\n",
        "\n",
        "        running_corrects = 0\n",
        "        for images, labels in val1_dataloader:\n",
        "          images = images.to(DEVICE)\n",
        "          labels = labels.to(DEVICE)\n",
        "          outputs = net(images)\n",
        "          _, preds = torch.max(outputs.data, 1)\n",
        "          running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "        cartoon_accuracy = running_corrects / float(len(val1_dataloader.dataset))\n",
        "        print (\"Cartoon Accuracy: \" + str(cartoon_accuracy))\n",
        "\n",
        "        running_corrects = 0\n",
        "        for images, labels in val2_dataloader:\n",
        "          images = images.to(DEVICE)\n",
        "          labels = labels.to(DEVICE)\n",
        "          outputs = net(images)\n",
        "          _, preds = torch.max(outputs.data, 1)\n",
        "          running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "        sketch_accuracy = running_corrects / float(len(val2_dataloader.dataset))\n",
        "        print (\"Sketch Accuracy: \" + str(sketch_accuracy))\n",
        "\n",
        "        accuracy = (cartoon_accuracy + sketch_accuracy) / 2\n",
        "        print (\"Accuracy: \" + str(accuracy))\n",
        "        if (accuracy > best_accuracy):\n",
        "          print(\"New best found!\")\n",
        "          best_accuracy = accuracy\n",
        "          best_net = net\n",
        "          best_hyper = [epoch, lr, ss]\n",
        "  print (\"best hyperparameters:\") \n",
        "  print (best_hyper) \n",
        "  return best_net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxYUli9d9uYQ",
        "colab_type": "text"
      },
      "source": [
        "**Training with DANN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcoQ5fD49yT_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def DANN_training_phase(rates, steps, alphas, train_dataloader, val1_dataloader):\n",
        "  accuracies = []\n",
        "  comb = []\n",
        "  for lr in rates:\n",
        "    for ss in steps:\n",
        "      for alpha in alphas:\n",
        "        print(\"------------------------------------------------------\")\n",
        "        print(\"Start training with LR: \" + str(lr) + \" and SS: \" + str(ss) + \" and ALPHA: \" + str(alpha))\n",
        "        print(\"------------------------------------------------------\")\n",
        "        comb.append({\"lr\":lr,\"ss\":ss,\"alpha\":alpha})\n",
        "        accuracy = []\n",
        "        net = alexnetDANN(pretrained=True)\n",
        "        net.classifier[6] = nn.Linear(4096, 7)\n",
        "        net.domain[1].weight.data = net.classifier[1].weight.data\n",
        "        net.domain[1].bias.data = net.classifier[1].bias.data\n",
        "        net.domain[4].weight.data = net.classifier[4].weight.data\n",
        "        net.domain[4].bias.data = net.classifier[4].bias.data\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss() # for classification, we use Cross Entropy\n",
        "        parameters_to_optimize = net.parameters() # In this case we optimize over all the parameters of AlexNet\n",
        "        optimizer = optim.SGD(parameters_to_optimize, lr=lr, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=ss, gamma=GAMMA)\n",
        "\n",
        "        net = net.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n",
        "        cudnn.benchmark # Calling this optimizes runtime\n",
        "\n",
        "        train_batches = len(train_dataloader)\n",
        "        val_batches = len(val1_dataloader)\n",
        "        n = max(train_batches, val_batches)\n",
        "\n",
        "        current_step = 0\n",
        "        # Start iterating over the epochs\n",
        "        for epoch in range(NUM_EPOCHS):\n",
        "          net = net.to(DEVICE)\n",
        "          net.train(True)\n",
        "          train_iter = iter(train_dataloader)\n",
        "          val_iter = iter(val1_dataloader)\n",
        "          print('Starting epoch {}/{}, LR = {}, SS = {}, alpha = {}'.format(epoch+1, NUM_EPOCHS, lr, ss, alpha))  \n",
        "          for i in range(n):\n",
        "            optimizer.zero_grad() # Zero-ing the gradients\n",
        "            if i < train_batches:\n",
        "              images,labels = next(train_iter)\n",
        "              images = images.to(DEVICE)\n",
        "              labels = labels.to(DEVICE)\n",
        "              net.train() # Sets module in training mode\n",
        "              outputs = net(images) # Forward pass to the network\n",
        "              loss = criterion(outputs, labels)\n",
        "              if math.isnan(loss):\n",
        "                print (\"Loss is NaN!\")\n",
        "                break\n",
        "              loss.backward()  # backward pass: computes gradients\n",
        "            \n",
        "              l = [0] * len(labels)\n",
        "              labels = torch.LongTensor(l)\n",
        "              images = images.to(DEVICE)\n",
        "              labels = labels.to(DEVICE)\n",
        "              net.train() # Sets module in training mode\n",
        "              outputs = net(images, alpha=0.1) # Forward pass to the network\n",
        "              loss = criterion(outputs, labels)\n",
        "              if math.isnan(loss):\n",
        "                print (\"Loss is NaN!\")\n",
        "                break\n",
        "              loss.backward()  # backward pass: computes gradients\n",
        "\n",
        "            if i < val_batches:\n",
        "              images,labels = next(val_iter)\n",
        "              l = [1] * len(labels)\n",
        "              labels = torch.LongTensor(l)\n",
        "              images = images.to(DEVICE)\n",
        "              labels = labels.to(DEVICE)\n",
        "              net.train() # Sets module in training mode\n",
        "              outputs = net(images, alpha=0.1) # Forward pass to the network\n",
        "              loss = criterion(outputs, labels)\n",
        "              if math.isnan(loss):\n",
        "                print (\"Loss is NaN!\")\n",
        "                break\n",
        "              loss.backward()  # backward pass: computes gradients\n",
        "\n",
        "            if current_step % LOG_FREQUENCY == 0:\n",
        "              print('Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "            current_step += 1\n",
        "            optimizer.step() # update weights based on accumulated gradients\n",
        "          if math.isnan(loss):\n",
        "            break\n",
        "          scheduler.step() # Step the scheduler\n",
        "\n",
        "          net = net.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n",
        "          net.train(False) # Set Network to evaluation mode\n",
        "\n",
        "          running_corrects = 0\n",
        "          for images, labels in val1_dataloader:\n",
        "            images = images.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "            outputs = net(images)\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "          single_accuracy = running_corrects / float(len(val1_dataloader.dataset))\n",
        "          accuracy.append(single_accuracy)\n",
        "          print (\"Single Accuracy: \" + str(single_accuracy))\n",
        "        print(accuracy)\n",
        "        accuracies.append(accuracy)      \n",
        "  return accuracies, comb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPP4dfcxZliZ",
        "colab_type": "text"
      },
      "source": [
        "**Final Training with DANN**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hht9fmATZl2d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def DANN_final_training(lr, ss, alpha, epochs, train_dataloader, test_dataloader):\n",
        "        net = alexnetDANN(pretrained=True)\n",
        "        net.classifier[6] = nn.Linear(4096, 7)\n",
        "        net.domain[1].weight.data = net.classifier[1].weight.data\n",
        "        net.domain[1].bias.data = net.classifier[1].bias.data\n",
        "        net.domain[4].weight.data = net.classifier[4].weight.data\n",
        "        net.domain[4].bias.data = net.classifier[4].bias.data\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss() # for classification, we use Cross Entropy\n",
        "        parameters_to_optimize = net.parameters() # In this case we optimize over all the parameters of AlexNet\n",
        "        optimizer = optim.SGD(parameters_to_optimize, lr=lr, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=ss, gamma=GAMMA)\n",
        "\n",
        "        net = net.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n",
        "        cudnn.benchmark # Calling this optimizes runtime\n",
        "\n",
        "        train_batches = len(train_dataloader)\n",
        "        test_batches = len(test_dataloader)\n",
        "        n = max(train_batches, test_batches)\n",
        "\n",
        "        current_step = 0\n",
        "        # Start iterating over the epochs\n",
        "        for epoch in range(epochs):\n",
        "          train_iter = iter(train_dataloader)\n",
        "          test_iter = iter(test_dataloader)\n",
        "          print('Starting epoch {}/{}, LR = {}, SS = {}, alpha = {}'.format(epoch+1, epochs, lr, ss, alpha))  \n",
        "          for i in range(n):\n",
        "            optimizer.zero_grad() # Zero-ing the gradients\n",
        "            if i < train_batches:\n",
        "              images,labels = next(train_iter)\n",
        "              images = images.to(DEVICE)\n",
        "              labels = labels.to(DEVICE)\n",
        "              net.train() # Sets module in training mode\n",
        "              outputs = net(images) # Forward pass to the network\n",
        "              loss = criterion(outputs, labels)\n",
        "              if math.isnan(loss):\n",
        "                print (\"Loss is NaN!\")\n",
        "                break\n",
        "              loss.backward()  # backward pass: computes gradients\n",
        "            \n",
        "              l = [0] * len(labels)\n",
        "              labels = torch.LongTensor(l)\n",
        "              images = images.to(DEVICE)\n",
        "              labels = labels.to(DEVICE)\n",
        "              net.train() # Sets module in training mode\n",
        "              outputs = net(images, alpha=0.1) # Forward pass to the network\n",
        "              loss = criterion(outputs, labels)\n",
        "              if math.isnan(loss):\n",
        "                print (\"Loss is NaN!\")\n",
        "                break\n",
        "              loss.backward()  # backward pass: computes gradients\n",
        "\n",
        "            if i < test_batches:\n",
        "              images,labels = next(test_iter)\n",
        "              l = [1] * len(labels)\n",
        "              labels = torch.LongTensor(l)\n",
        "              images = images.to(DEVICE)\n",
        "              labels = labels.to(DEVICE)\n",
        "              net.train() # Sets module in training mode\n",
        "              outputs = net(images, alpha=0.1) # Forward pass to the network\n",
        "              loss = criterion(outputs, labels)\n",
        "              if math.isnan(loss):\n",
        "                print (\"Loss is NaN!\")\n",
        "                break\n",
        "              loss.backward()  # backward pass: computes gradients\n",
        "\n",
        "            if current_step % LOG_FREQUENCY == 0:\n",
        "              print('Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "            current_step += 1\n",
        "            optimizer.step() # update weights based on accumulated gradients\n",
        "          if math.isnan(loss):\n",
        "            break\n",
        "          scheduler.step() # Step the scheduler\n",
        "        return net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsHFI-GAJd69",
        "colab_type": "text"
      },
      "source": [
        "**Test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EO3HV5pqJg1o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_phase(net, test_dataloader):\n",
        "  net = net.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n",
        "  net.train(False) # Set Network to evaluation mode\n",
        "\n",
        "  running_corrects = 0\n",
        "  for images, labels in tqdm(test_dataloader):\n",
        "    images = images.to(DEVICE)\n",
        "    labels = labels.to(DEVICE)\n",
        "    outputs = net(images)\n",
        "    _, preds = torch.max(outputs.data, 1)\n",
        "    running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "  accuracy = running_corrects / float(len(test_dataloader.dataset))\n",
        "  print('Test Accuracy: {}'.format(accuracy))\n",
        "  return accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwuclukPf8hY",
        "colab_type": "text"
      },
      "source": [
        "**Main**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvVTLrfNf81U",
        "colab_type": "code",
        "outputId": "4b324941-fd0d-480b-e321-9a44cbd73dbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        }
      },
      "source": [
        "best_net = training_phase([0.001, 0.005, 0.01], [20, 10], train_dataloader, val1_dataloader, val2_dataloader)\n",
        "accuracy = test_phase(best_net, test_dataloader)\n",
        "print(\"Accuracy without DANN: \" + str(accuracy))\n",
        "\n",
        "cartoon_accuracies, comb = DANN_training_phase([0.001,0.0005,0.0001], [10,20], [0.5,0.3,0.1], train_dataloader, val1_dataloader)\n",
        "print (\"CARTOON Accuracies:\")\n",
        "print (cartoon_accuracies)\n",
        "\n",
        "sketch_accuracies, comb = DANN_training_phase([0.001,0.0005,0.0001], [10,20], [0.5,0.3,0.1], train_dataloader, val2_dataloader)\n",
        "print (\"SKETCH Accuracies:\")\n",
        "print (sketch_accuracies)\n",
        "\n",
        "accuracies = []\n",
        "for i in range(len(cartoon_accuracies)):\n",
        "  average = []\n",
        "  n = min(len(cartoon_accuracies[i]), len(sketch_accuracies[i]))\n",
        "  for j in range(n):\n",
        "    average.append((cartoon_accuracies[i][j] + sketch_accuracies[i][j])/2)\n",
        "  accuracies.append(average)\n",
        "\n",
        "print (\"Average Accuracies:\")\n",
        "print (accuracies)\n",
        "\n",
        "maximum = 0\n",
        "for i in range(len(accuracies)):\n",
        "  for j in range(5, len(accuracies[i])):\n",
        "    if accuracies[i][j] > maximum:\n",
        "      maximum = accuracies[i][j]\n",
        "      max_i = i\n",
        "      max_j = j\n",
        "\n",
        "print (\"Highest Accuracy found:\" + str(accuracies[max_i][max_j]))\n",
        "print (\"Best Hyperparameters: \" + str(comb[max_i]))\n",
        "print (\"Best number of Epochs: \" + str(max_j))\n",
        "\n",
        "best_net = DANN_final_training(comb[max_i][\"lr\"], comb[max_i][\"ss\"], comb[max_i][\"alpha\"], max_j+1, train_dataloader, test_dataloader)\n",
        "accuracy = test_phase(best_net, test_dataloader)\n",
        "print(\"Accuracy with DANN: \" + str(accuracy))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.2019067435894986, 0.21595006111030518, 0.2097119346210944, 0.2002244728747556, 0.2243733587735201, 0.21732444577253068, 0.23732227194824168, 0.2419860045674198, 0.24537714874170102, 0.26246257156681263, 0.26068094774395695, 0.25588631876212325, 0.251727984002738, 0.25062022399293954, 0.2509195863088594, 0.24990151555294188, 0.24867013421681955, 0.24867013421681955, 0.24833320231029093, 0.24829199520151635, 0.24829199520151635, 0.24829199520151635, 0.24829199520151635, 0.24829199520151635, 0.24829199520151635, 0.24829199520151635, 0.24829199520151635, 0.24829199520151635, 0.24829199520151635, 0.24807868462131155], [0.19742841581414822, 0.1990755057561825, 0.17983515202002787, 0.1896110635277889, 0.19474989945248294, 0.20056140478128417, 0.21858731607187998, 0.2065013633635251, 0.2178419505957712, 0.2087435404192332, 0.20972040406637613, 0.21273704674352, 0.21477682577352097, 0.2145186705663757, 0.21494165420861938, 0.21498286131739397, 0.2160045695914774, 0.21557794843106784, 0.21613182843596707, 0.21613182843596707, 0.21587731074698768, 0.21587731074698768, 0.21613182843596707, 0.21587731074698768, 0.21587731074698768, 0.21609062132719248, 0.21609062132719248, 0.2159633624827028, 0.21587731074698768, 0.21587731074698768], [0.24764603712483615, 0.21443261883066062, 0.2254338310471622, 0.2380215440971441, 0.22924432134552122, 0.2872271209879803, 0.25693832159048363, 0.2932676813785998, 0.2381937018598902, 0.290160155038625, 0.290249844292506, 0.28868516856802096, 0.2871204928435359, 0.28453410884496744, 0.2822022425353784, 0.28059272218395287, 0.27898320183252734, 0.2773288368541614, 0.2769058532119177, 0.27635561072518433, 0.27656892130538907, 0.2761010930362049, 0.2761010930362049, 0.27584657534722556, 0.2762731965076351, 0.27614593766314544, 0.27614593766314544, 0.2760186788186557, 0.2760186788186557, 0.2758053682384509], [0.20343748724154076, 0.18346387499272496, 0.18482251517333698, 0.17898435280842462, 0.18507703286231636, 0.18041085713392235, 0.19833377779823957, 0.19375245939661068, 0.19835196538906893, 0.193505216743963, 0.20764066662786648, 0.230932672687646, 0.21764927071561166, 0.24233265461949605, 0.20579481617829093, 0.24023348088989113, 0.2354873340531638, 0.22551266203786147, 0.22389950416827006, 0.22730883593338064, 0.22684464518236236, 0.22578172979950434, 0.22467396978970583, 0.2249321249968511, 0.2240825201941979, 0.22412736482113832, 0.22395889886787404, 0.22493940003318286, 0.22404495060358914, 0.22400374349481453], [0.29600320362196914, 0.2288286670309252, 0.2245139732817233, 0.22697673595396792, 0.21842129322783155, 0.22761303017641638, 0.2298249126778475, 0.2374640808653949, 0.23485587175783118, 0.26367576531210557, 0.2403764842159943, 0.2425205025725397, 0.28244584766986014, 0.2836093648610968, 0.2729802110325166, 0.26806559824252496, 0.2719573083494832, 0.27136222123580933, 0.2695018207135703, 0.26443448645192785, 0.264135124136008, 0.2640939170272334, 0.2647338487678477, 0.26469264165907314, 0.2646514345502985, 0.2648235380217287, 0.26525015918213823, 0.26559072860683275, 0.26559072860683275, 0.2658040391870375], [0.20447857751540355, 0.19912279349233886, 0.1993785055902682, 0.2005311102270072, 0.18724526514575698, 0.19102426648088902, 0.2511946804065681, 0.25443554621841435, 0.25041902037618236, 0.2326997464378382, 0.23576123374192254, 0.23737075409334807, 0.2356266998611011, 0.23312636759824773, 0.23195074344356353, 0.22936799696316096, 0.23697687059643135, 0.24302834354154848, 0.23706655985031233, 0.236426628109698, 0.2362133175294932, 0.23557338578887888, 0.23591395521357336, 0.2354873340531638, 0.23595516232234798, 0.2365090423272472, 0.23586911058663287, 0.23527038595479316, 0.23539764479928282, 0.2359066801772416], [0.20289451979113915, 0.18487946676372508, 0.18542607173229256, 0.1879857986947499, 0.18708407422882442, 0.18434860627677105, 0.18341294973840272, 0.188012455730861, 0.19099880385372792, 0.19487232636985677, 0.19474506752536708, 0.19508563695006154, 0.194659015789652, 0.19474506752536708, 0.19508563695006154, 0.19627581117740925, 0.19618975944169414, 0.1967436394465934, 0.19653032886638863, 0.19712905349822835, 0.19712905349822835, 0.19712905349822835, 0.19712905349822835, 0.19712905349822835, 0.19712905349822835, 0.19712905349822835, 0.19712905349822835, 0.19734236407843314, 0.19746962292292283, 0.19746962292292283], [0.23115081519496664, 0.23006607470311335, 0.21369936031799944, 0.20907319728943002, 0.2085193172845308, 0.20997855927352138, 0.21172261350576835, 0.20922228124291498, 0.2089677635539356, 0.2105094197604754, 0.2105094197604754, 0.2107639374494548, 0.21123176571863894, 0.21110086935598338, 0.21173716357843184, 0.2110972318378175, 0.2110111801021024, 0.2112244906823072, 0.21169231895149138, 0.21169231895149138, 0.21203288837618584, 0.21203288837618584, 0.21203288837618584, 0.21203288837618584, 0.21203288837618584, 0.21203288837618584, 0.21203288837618584, 0.21203288837618584, 0.21203288837618584, 0.21203288837618584], [0.2842905580018016, 0.2637267448577437, 0.2460401542915765, 0.2358497285868535, 0.2362024049749956, 0.2390881512894839, 0.23937296353274026, 0.24316043431315404, 0.24733695666336863, 0.26101304772336964, 0.25919021679173937, 0.25808609430010676, 0.25710923065296387, 0.25723648949745354, 0.25723648949745354, 0.2568959200727591, 0.256809868337044, 0.256809868337044, 0.2577455248754123, 0.2577455248754123, 0.25787278371990197, 0.25787278371990197, 0.25787278371990197, 0.25787278371990197, 0.2577455248754123, 0.2577455248754123, 0.2577455248754123, 0.2577455248754123, 0.2577455248754123, 0.2577455248754123], [0.33257622283588295, 0.2995652025674146, 0.27890561954209403, 0.27878563573393605, 0.2778948238225082, 0.27671556214965815, 0.2794922372104861, 0.28782709432008596, 0.29454879356009445, 0.29799205739764784, 0.29350406576806576, 0.2977896593719407, 0.2947572722131833, 0.30151290352563465, 0.29790844877114864, 0.2962807408288938, 0.29211876855134267, 0.29781875951726766, 0.29151157447422116, 0.306288150507689, 0.3054385457050357, 0.30335755956626015, 0.3023394888103426, 0.300939641520956, 0.3008123826764663, 0.3002172955627925, 0.2994501049776884, 0.29979067440238294, 0.2989822767085043, 0.2988962249727892], [0.24204301044912382, 0.24777568478722584, 0.23806036238801873, 0.24405607815169778, 0.2443627155039494, 0.24223324722006748, 0.2431689037584358, 0.24861073951721557, 0.2501414831692577, 0.24787020596822262, 0.2536041290065905, 0.2546367498351716, 0.250309949122522, 0.2554148529747732, 0.2561444739692685, 0.25950532358927275, 0.2620020183339602, 0.2667966473157939, 0.26605975128496684, 0.2620626074425142, 0.262275918022719, 0.2627025391831285, 0.26312916034353806, 0.26355578150394765, 0.26376909208415245, 0.264195713244562, 0.2648356449851763, 0.26474959324946123, 0.2653034732543605, 0.26521742151864536], [0.2302575601743229, 0.2379536799522584, 0.24358216925513182, 0.23864573135614497, 0.23731374821164408, 0.2387960097185799, 0.24135209916287134, 0.24430815273146125, 0.2440087904155414, 0.24380639238983426, 0.2459285856373844, 0.25056566122045143, 0.2528599579394317, 0.2572316032790217, 0.25689103385432727, 0.2648671339484033, 0.26045428150003863, 0.26321276897003726, 0.2710458657380101, 0.27575808050229456, 0.2761434945539295, 0.2760610803363803, 0.2749533203265818, 0.2749533203265818, 0.2752078380155612, 0.2756756662847454, 0.2755035628133152, 0.27580292512923504, 0.27580292512923504, 0.2752490451243358], [0.12746992912594457, 0.1698241591143827, 0.18807429353968086, 0.20523241243679405, 0.20915924902514513, 0.21161109914289214, 0.2127637037796311, 0.21357573899167562, 0.21217952922045488, 0.2102294394443349, 0.2101021805998452, 0.2102294394443349, 0.2102294394443349, 0.21014338770861982, 0.2099749217553555, 0.20980281828392533, 0.20980281828392533, 0.20993007712841505, 0.21001612886413012, 0.21048395713331428, 0.21048395713331428, 0.21048395713331428, 0.21048395713331428, 0.21048395713331428, 0.2102706465531095, 0.21014338770861982, 0.21014338770861982, 0.21014338770861982, 0.21014338770861982, 0.21014338770861982], [0.22365095852404063, 0.24458085801127, 0.239942588019253, 0.2343359238253748, 0.22595622208883448, 0.2171498991918846, 0.21413325651474074, 0.21005006093657297, 0.20830600670432603, 0.2075909357824942, 0.20733641809351483, 0.20699584866882037, 0.20656922750841084, 0.20656922750841084, 0.20631470981943142, 0.2064007615551465, 0.20614624386616712, 0.20589172617718773, 0.20576446733269804, 0.20576446733269804, 0.20576446733269804, 0.20576446733269804, 0.20576446733269804, 0.20576446733269804, 0.20576446733269804, 0.20576446733269804, 0.20576446733269804, 0.20576446733269804, 0.20576446733269804, 0.20576446733269804], [0.13367043173323073, 0.18665984188631485, 0.20390281811019312, 0.20796782609753153, 0.20963310363039514, 0.2100597247908047, 0.21211041637530326, 0.20959917155795227, 0.2090937736981594, 0.2091386183250999, 0.20901135948061017, 0.20922467006081497, 0.20935192890530466, 0.20922467006081497, 0.20854353121142602, 0.20820296178673156, 0.20841627236693633, 0.20841627236693633, 0.2088428935273459, 0.20905620410755066, 0.20905620410755066, 0.20905620410755066, 0.20905620410755066, 0.20905620410755066, 0.20905620410755066, 0.20905620410755066, 0.208928945263061, 0.2087156346828562, 0.2087156346828562, 0.2085883758383665], [0.21126330897318182, 0.27274154640778253, 0.31166825704028067, 0.32038489068334963, 0.31559873114679765, 0.3027661642620681, 0.29155403028326166, 0.28686119751875655, 0.28119513862527434, 0.27910687745016705, 0.27557028684056684, 0.274294060877504, 0.27284573144301105, 0.2715658679617824, 0.270248434889945, 0.26824622545055277, 0.2656937735244272, 0.26458601351462874, 0.2625838040752365, 0.2609670086874792, 0.2609670086874792, 0.2609670086874792, 0.2609670086874792, 0.26109426753196885, 0.26109426753196885, 0.2608809569517641, 0.2608809569517641, 0.2608809569517641, 0.2610082157962538, 0.26122152637645857], [0.1632030073914369, 0.1932276795370384, 0.19779689097521969, 0.1937888345782694, 0.19046191703070803, 0.18640901600681725, 0.18427591020476947, 0.18389049615313452, 0.18538367021456797, 0.18457163500252344, 0.18478494558272823, 0.18448558326680836, 0.1855072915408918, 0.1873410350270197, 0.18819427734783883, 0.18904751966865793, 0.1901552796784564, 0.19032374563172072, 0.19075036679213028, 0.19032374563172072, 0.19053705621192552, 0.19053705621192552, 0.19075036679213028, 0.19100488448110967, 0.19100488448110967, 0.19100488448110967, 0.1907915739009049, 0.1907915739009049, 0.19100488448110967, 0.19100488448110967], [0.18126046193657558, 0.2037416814845765, 0.21129843545457466, 0.21734382777231004, 0.2187072998800379, 0.2210621857075722, 0.22249596506940167, 0.2228256219395985, 0.22380248558674148, 0.226179196523271, 0.23025875458327288, 0.23332024188735723, 0.23403895032735494, 0.23488855513000817, 0.23714164474021387, 0.23760947300939805, 0.23994861435531886, 0.23995225187348473, 0.2405885460959332, 0.24241501454572933, 0.24262832512593413, 0.24262832512593413, 0.24262832512593413, 0.24275558397042382, 0.2428828428149135, 0.2428828428149135, 0.2432646193483826, 0.24360518877307707, 0.2440318099334866, 0.2440318099334866]]\n",
            "9\n",
            "19\n",
            "0.306288150507689\n",
            "0.306288150507689\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-c3cfaeec0c49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maccuracies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmax_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmax_j\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Iperparametri: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmax_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoche: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_j\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m '''\n",
            "\u001b[0;31mNameError\u001b[0m: name 'comb' is not defined"
          ]
        }
      ]
    }
  ]
}